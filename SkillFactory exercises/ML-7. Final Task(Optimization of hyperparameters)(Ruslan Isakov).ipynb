{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2944,"status":"ok","timestamp":1666967182422,"user":{"displayName":"john appleseedoff","userId":"04252315395653508103"},"user_tz":-180},"id":"ACpQWI8Q-uNz"},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","\n","from sklearn import linear_model \n","from sklearn import ensemble \n","from sklearn import metrics \n","from sklearn import model_selection\n","import hyperopt\n","from hyperopt import hp, fmin, tpe, Trials\n","import optuna"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"elapsed":38845,"status":"ok","timestamp":1666967225777,"user":{"displayName":"john appleseedoff","userId":"04252315395653508103"},"user_tz":-180},"id":"I97JPTku-uN7","outputId":"7cc3b0ef-d49e-4e82-a4ee-425d8f853688"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Activity</th>\n","      <th>D1</th>\n","      <th>D2</th>\n","      <th>D3</th>\n","      <th>D4</th>\n","      <th>D5</th>\n","      <th>D6</th>\n","      <th>D7</th>\n","      <th>D8</th>\n","      <th>D9</th>\n","      <th>...</th>\n","      <th>D1767</th>\n","      <th>D1768</th>\n","      <th>D1769</th>\n","      <th>D1770</th>\n","      <th>D1771</th>\n","      <th>D1772</th>\n","      <th>D1773</th>\n","      <th>D1774</th>\n","      <th>D1775</th>\n","      <th>D1776</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0.497009</td>\n","      <td>0.10</td>\n","      <td>0.0</td>\n","      <td>0.132956</td>\n","      <td>0.678031</td>\n","      <td>0.273166</td>\n","      <td>0.585445</td>\n","      <td>0.743663</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.366667</td>\n","      <td>0.606291</td>\n","      <td>0.05</td>\n","      <td>0.0</td>\n","      <td>0.111209</td>\n","      <td>0.803455</td>\n","      <td>0.106105</td>\n","      <td>0.411754</td>\n","      <td>0.836582</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0.033300</td>\n","      <td>0.480124</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.209791</td>\n","      <td>0.610350</td>\n","      <td>0.356453</td>\n","      <td>0.517720</td>\n","      <td>0.679051</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0.538825</td>\n","      <td>0.00</td>\n","      <td>0.5</td>\n","      <td>0.196344</td>\n","      <td>0.724230</td>\n","      <td>0.235606</td>\n","      <td>0.288764</td>\n","      <td>0.805110</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.100000</td>\n","      <td>0.517794</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.494734</td>\n","      <td>0.781422</td>\n","      <td>0.154361</td>\n","      <td>0.303809</td>\n","      <td>0.812646</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1777 columns</p>\n","</div>"],"text/plain":["   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n","0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n","1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n","2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n","3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n","4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n","\n","         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n","0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n","1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n","2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n","3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n","4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n","\n","   D1774  D1775  D1776  \n","0      0      0      0  \n","1      0      1      0  \n","2      0      0      0  \n","3      0      0      0  \n","4      0      0      0  \n","\n","[5 rows x 1777 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load data\n","data = pd.read_csv(\"Data/train_sem09.csv\")\n","data.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1666967234278,"user":{"displayName":"john appleseedoff","userId":"04252315395653508103"},"user_tz":-180},"id":"7k9W7Yn3-uN9"},"outputs":[],"source":["# Prepare data\n","X = data.drop([\"Activity\"], axis=1)\n","y = data[\"Activity\"]\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(\n","                                                    X, y, \n","                                                    stratify=y, \n","                                                    test_size=0.2, \n","                                                    random_state=42\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8682,"status":"ok","timestamp":1666967256864,"user":{"displayName":"john appleseedoff","userId":"04252315395653508103"},"user_tz":-180},"id":"kRJAUEZl-uN-","outputId":"629fd222-3123-41d7-c04e-df6ef7c9681d"},"outputs":[{"name":"stdout","output_type":"stream","text":["LogisticRegression accuracy score on test data: 0.75\n","LogisticRegression f1-score on test data: 0.78\n","\n","RandomForestClassifier f1-score on train data: 1.0\n","RandomForestClassifier f1-score on test data: 0.8\n"]}],"source":["# Create models without optimization\n","# LogisticRegression\n","model_log_r = linear_model.LogisticRegression(max_iter=1000)\n","model_log_r.fit(X_train, y_train)\n","print(\n","    f\"LogisticRegression accuracy score on test data: \\\n","{np.round(model_log_r.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = model_log_r.predict(X_test)\n","print(\n","    f\"LogisticRegression f1-score on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")\n","print()\n","\n","# RandomForestClassifier\n","model_rfc = ensemble.RandomForestClassifier(random_state=42)\n","model_rfc.fit(X_train, y_train)\n","y_train_pred = model_rfc.predict(X_train)\n","print(\n","    f\"RandomForestClassifier f1-score on train data: \\\n","{np.round(metrics.f1_score(y_train, y_train_pred), 2)}\"\n",")\n","y_test_pred = model_rfc.predict(X_test)\n","print(\n","    f\"RandomForestClassifier f1-score on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":2117043,"status":"error","timestamp":1666970033904,"user":{"displayName":"john appleseedoff","userId":"04252315395653508103"},"user_tz":-180},"id":"988JTU_k-uOA","outputId":"02b61d6b-7457-4ce1-bf42-73a10e0a166f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["LogisticRegression accuracy score after GridSearchCV on test data: 0.76\n","LogisticRegression f1-score after GridSearchCV on test data: 0.78\n","LogisticRegression GridSearchCV best hypereparameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n","LogisticRegression GridSearchCV accuracy score on cross-validation: 0.76\n"]}],"source":["# GridSearchCV LogisticRegression\n","\n","param_grid_lr = {\n","    \"penalty\": [\"l2\", \"none\"],\n","    \"solver\": [\"lbfgs\", \"sag\"], \n","    \"C\": [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n","}\n","\n","grid_search_lr = model_selection.GridSearchCV(\n","    estimator=linear_model.LogisticRegression(random_state=42, max_iter=1000),\n","    param_grid=param_grid_lr,\n","    cv=5,\n","    n_jobs=-1\n",")\n","\n","grid_search_lr.fit(X_train, y_train)\n","print(\n","    f\"LogisticRegression accuracy score after GridSearchCV on test data: \\\n","{np.round(grid_search_lr.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = grid_search_lr.predict(X_test)\n","print(f\"LogisticRegression f1-score after GridSearchCV on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")\n","print(f\"LogisticRegression GridSearchCV best hypereparameters: {grid_search_lr.best_params_}\")\n","print(\n","    f\"LogisticRegression GridSearchCV accuracy score on cross-validation: \\\n","{np.round(grid_search_lr.best_score_, 2)}\"\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RandomForestClassifier f1-score after GridSearchCV on train data: 0.94\n","RandomForestClassifier accuracy score after GridSearchCV on test data: 0.78\n","RandomForestClassifier f1-score after GridSearchCV on test data: 0.8\n","RandomForestClassifier GridSearchCV best hyperparameters: {'max_depth': 20, 'min_samples_leaf': 5, 'n_estimators': 350}\n","RandomForestClassifier GridSearchCV accuracy score on cross-validation: 0.79\n"]}],"source":["# GridSearchCV RandomForestClassifier\n","\n","param_grid_rfc = {\n","    \"n_estimators\": list(np.linspace(50, 400, 8, dtype=int)),\n","    \"min_samples_leaf\": [5, 7],\n","    \"max_depth\": list(np.linspace(20, 40, 4, dtype=int))\n","}\n","\n","grid_search_rfc = model_selection.GridSearchCV(\n","    estimator=ensemble.RandomForestClassifier(random_state=42),\n","    param_grid=param_grid_rfc,\n","    cv=5,\n","    n_jobs=-1\n",")\n","\n","grid_search_rfc.fit(X_train, y_train)\n","y_train_pred = grid_search_rfc.predict(X_train)\n","print(\n","    f\"RandomForestClassifier f1-score after GridSearchCV on train data: \\\n","{np.round(metrics.f1_score(y_train, y_train_pred), 2)}\"\n",")\n","print(\n","    f\"RandomForestClassifier accuracy score after GridSearchCV on test data: \\\n","{np.round(grid_search_rfc.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = grid_search_rfc.predict(X_test)\n","print(\n","    f\"RandomForestClassifier f1-score after GridSearchCV on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")\n","print(f\"RandomForestClassifier GridSearchCV best hyperparameters: {grid_search_rfc.best_params_}\")\n","print(\n","    f\"RandomForestClassifier GridSearchCV accuracy score on cross-validation: \\\n","{np.round(grid_search_rfc.best_score_, 2)}\"\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VDgQ6jh_DRR1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","/home/ri/anaconda3/envs/SFDS/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["LogisticRegression accuracy score after RandomSearchCV on test data: 0.76\n","LogisticRegression f1-score after RandomSearchCV on test data: 0.78\n","LogisticRegression RandomSearchCV best hyperparameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.01}\n","LogisticRegression RandomSearchCV accuracy score on cross-validation: 0.76\n"]}],"source":["# RandomSearchCV LogisticRegression\n","\n","param_distributions_lr = {\n","    \"penalty\": [\"l2\", \"none\"],\n","    \"solver\": [\"lbfgs\", \"sag\"],\n","    \"C\": list(np.linspace(0.01, 1, 10, dtype=float))\n","}\n","\n","random_search_lr = model_selection.RandomizedSearchCV(\n","    estimator=linear_model.LogisticRegression(random_state=42, max_iter=1000), \n","    param_distributions=param_distributions_lr, \n","    cv=5, \n","    n_iter = 10, \n","    n_jobs = -1\n",")  \n","random_search_lr.fit(X_train, y_train) \n","print(\n","    f\"LogisticRegression accuracy score after RandomSearchCV on test data: \\\n","{np.round(random_search_lr.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = random_search_lr.predict(X_test)\n","print(\n","    f\"LogisticRegression f1-score after RandomSearchCV on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")\n","print(f\"LogisticRegression RandomSearchCV best hyperparameters: {random_search_lr.best_params_}\")\n","print(\n","    f\"LogisticRegression RandomSearchCV accuracy score on cross-validation: \\\n","{np.round(random_search_lr.best_score_, 2)}\"\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RandomForestClassifier f1-score after RandomSearchCV on train data: 0.95\n","RandomForestClassifier accuracy score after RandomSearchCV on train data: 0.78\n","RandomForestClassifier f1-score after RandomSearchCV on test data: 0.8\n","RandomForestClassifier RandomSearchCV best hyperparameters: {'n_estimators': 170, 'min_samples_leaf': 5, 'max_depth': 22}\n","RandomForestClassifier RandomSearchCV accuracy score on cross-validation: 0.79\n"]}],"source":["# RandomSearchCV RandomForestClassifier\n","\n","param_distributions_rfc = {\n","    \"n_estimators\": list(range(80, 200, 30)),\n","    \"min_samples_leaf\": [5],\n","    \"max_depth\": list(np.linspace(20, 40, 10, dtype=int))\n","}\n","            \n","random_search_rfc = model_selection.RandomizedSearchCV(\n","    estimator=ensemble.RandomForestClassifier(random_state=42), \n","    param_distributions=param_distributions_rfc, \n","    cv=5,\n","    n_iter = 10, \n","    n_jobs = -1\n",")  \n","random_search_rfc.fit(X_train, y_train) \n","y_train_pred = random_search_rfc.predict(X_train)\n","print(\n","    f\"RandomForestClassifier f1-score after RandomSearchCV on train data: \\\n","{np.round(metrics.f1_score(y_train, y_train_pred), 2)}\"\n",")\n","print(\n","    f\"RandomForestClassifier accuracy score after RandomSearchCV on train data: \\\n","{np.round(random_search_rfc.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = random_search_rfc.predict(X_test)\n","print(\n","    f\"RandomForestClassifier f1-score after RandomSearchCV on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")\n","print(f\"RandomForestClassifier RandomSearchCV best hyperparameters: {random_search_rfc.best_params_}\")\n","print(\n","    f\"RandomForestClassifier RandomSearchCV accuracy score on cross-validation: \\\n","{np.round(random_search_rfc.best_score_, 2)}\"\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["TPE is being used as the default algorithm.\n"]},{"name":"stdout","output_type":"stream","text":["100%|██████████| 20/20 [02:49<00:00,  8.47s/trial, best loss: -0.7929826808799139]\n","LogisticRegression Hyperopt best hyperparameters: {'C': 0.02692572304826705, 'max_iter': 1515.1584720707733, 'solver': 1}\n","LogisticRegression f1_score after Hyperpopt on train data: 0.84\n","LogisticRegression accuracy after Hyperpopt on test data: 0.75\n","LogisticRegression f1_score after Hyperpopt on test data: 0.78\n"]}],"source":["# Hyperpopt LogisticRegression\n","\n","# space for hyperparameters search\n","solvers = [\"sag\", \"lbfgs\"]\n","space_lr = {\n","    \"solver\" : hp.choice(label=\"solver\", options=solvers),\n","    \"max_iter\" : hp.uniform(label=\"max_iter\", low=700, high=1600),\n","    \"C\" : hp.uniform(label=\"C\", low=0.01, high=1.0)\n","}\n","\n","random_state = 42\n","def hyperopt_lr(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n","    # combination of hyperparameters\n","    params = {\n","        \"solver\": params[\"solver\"],\n","        \"max_iter\": int(params[\"max_iter\"]), \n","        \"C\": float(params[\"C\"])\n","    }\n","\n","    # create and train model with cross-validation\n","    model = linear_model.LogisticRegression(**params, random_state=random_state)\n","    score = model_selection.cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n","    \n","    # minimize metric\n","    return -score\n","\n","trials_lr = Trials() # logging\n","best=fmin(\n","    hyperopt_lr, \n","    space=space_lr, \n","    max_evals=20, \n","    trials=trials_lr,\n","    rstate=np.random.default_rng(random_state)\n",")\n","print(f\"LogisticRegression Hyperopt best hyperparameters: {best}\")\n","\n","# Get accuracy and f1-score for LogisticRegression model\n","solver = solvers[best[\"solver\"]]\n","\n","model_lr_hopt = linear_model.LogisticRegression(\n","    random_state=random_state,\n","    solver=solver,\n","    max_iter=int(best[\"max_iter\"]),\n","    C=float(best[\"C\"]),\n",")\n","model_lr_hopt.fit(X_train, y_train)\n","y_train_pred = model_lr_hopt.predict(X_train)\n","print(\n","    f\"LogisticRegression f1_score after Hyperpopt on train data: \\\n","{np.round(metrics.f1_score(y_train, y_train_pred), 2)}\"\n",")\n","print(\n","    f\"LogisticRegression accuracy after Hyperpopt on test data: \\\n","{np.round(model_lr_hopt.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = model_lr_hopt.predict(X_test)\n","print(\n","    f\"LogisticRegression f1_score after Hyperpopt on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100%|██████████| 20/20 [00:31<00:00,  1.59s/trial, best loss: -0.8160803811393121]\n","RandomForestClassifier Hyperopt best hyperparameters: {'max_depth': 18.0, 'min_samples_leaf': 2.0, 'n_estimators': 103.0}\n","RandomForestClassifier f1_score after Hyperpopt on train data: 0.99\n","RandomForestClassifier accuracy after Hyperpopt on test data: 0.79\n","RandomForestClassifier f1_score after Hyperpopt on test data: 0.81\n"]}],"source":["# Hyperopt RandomForestClassifier\n","\n","space_rfc={\n","    \"n_estimators\": hp.quniform(\"n_estimators\", 100, 200, 1),\n","    \"max_depth\": hp.quniform(\"max_depth\", 15, 26, 1),\n","    \"min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 2, 10, 1)\n","}\n","\n","random_state = 42\n","def hyperopt_rfc(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n","    # combination of hyperparameters\n","    params = {\n","        \"n_estimators\": int(params[\"n_estimators\"]), \n","        \"max_depth\": int(params[\"max_depth\"]), \n","        \"min_samples_leaf\": int(params[\"min_samples_leaf\"])\n","    }\n","\n","    # create and train model with cross-validation\n","    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n","    score = model_selection.cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n","\n","    # minimize metric\n","    return -score\n","\n","trials_rfc = Trials() # logging\n","\n","best=fmin(\n","    hyperopt_rfc,\n","    space=space_rfc,\n","    algo=tpe.suggest,\n","    max_evals=20,\n","    trials=trials_rfc,\n","    rstate=np.random.default_rng(random_state)\n",")\n","\n","print(f\"RandomForestClassifier Hyperopt best hyperparameters: {best}\")\n","\n","# Get accuracy and f1-score for RandomForestClassifier model\n","model_rfc_hopt = ensemble.RandomForestClassifier(\n","    random_state=random_state, \n","    n_estimators=int(best['n_estimators']),\n","    max_depth=int(best['max_depth']),\n","    min_samples_leaf=int(best['min_samples_leaf'])\n",")\n","model_rfc_hopt.fit(X_train, y_train)\n","y_train_pred = model_rfc_hopt.predict(X_train)\n","print(\n","    f\"RandomForestClassifier f1_score after Hyperpopt on train data: \\\n","{np.round(metrics.f1_score(y_train, y_train_pred), 2)}\"\n",")\n","print(\n","    f\"RandomForestClassifier accuracy after Hyperpopt on test data: \\\n","{np.round(model_rfc_hopt.score(X_test, y_test), 2)}\"\n",")\n","y_test_pred = model_rfc_hopt.predict(X_test)\n","print(\n","    f\"RandomForestClassifier f1_score after Hyperpopt on test data: \\\n","{np.round(metrics.f1_score(y_test, y_test_pred), 2)}\"\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-10-30 00:58:23,497]\u001b[0m A new study created in memory with name: LogisticRefression\u001b[0m\n","\u001b[32m[I 2022-10-30 00:58:44,718]\u001b[0m Trial 0 finished with value: 0.8865291262135923 and parameters: {'solver': 'saga', 'max_iter': 910, 'C': 0.6290324966249393}. Best is trial 0 with value: 0.8865291262135923.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:04,573]\u001b[0m Trial 1 finished with value: 0.8813353566009106 and parameters: {'solver': 'sag', 'max_iter': 720, 'C': 0.44081449105049414}. Best is trial 0 with value: 0.8865291262135923.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:17,563]\u001b[0m Trial 2 finished with value: 0.84688995215311 and parameters: {'solver': 'saga', 'max_iter': 1080, 'C': 0.06643987564083981}. Best is trial 0 with value: 0.8865291262135923.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:18,494]\u001b[0m Trial 3 finished with value: 0.8771610555050044 and parameters: {'solver': 'lbfgs', 'max_iter': 1580, 'C': 0.34257490472396446}. Best is trial 0 with value: 0.8865291262135923.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:31,759]\u001b[0m Trial 4 finished with value: 0.84688995215311 and parameters: {'solver': 'saga', 'max_iter': 860, 'C': 0.07221482889165823}. Best is trial 0 with value: 0.8865291262135923.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:45,854]\u001b[0m Trial 5 finished with value: 0.8882817243472981 and parameters: {'solver': 'sag', 'max_iter': 1450, 'C': 0.7271573440683862}. Best is trial 5 with value: 0.8882817243472981.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:47,176]\u001b[0m Trial 6 finished with value: 0.8837772397094431 and parameters: {'solver': 'lbfgs', 'max_iter': 890, 'C': 0.5117519898296878}. Best is trial 5 with value: 0.8882817243472981.\u001b[0m\n","\u001b[32m[I 2022-10-30 00:59:48,801]\u001b[0m Trial 7 finished with value: 0.8857229463473779 and parameters: {'solver': 'lbfgs', 'max_iter': 720, 'C': 0.6217297033406117}. Best is trial 5 with value: 0.8882817243472981.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:00:06,509]\u001b[0m Trial 8 finished with value: 0.8831011508176863 and parameters: {'solver': 'saga', 'max_iter': 740, 'C': 0.498920236177518}. Best is trial 5 with value: 0.8882817243472981.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:00:07,679]\u001b[0m Trial 9 finished with value: 0.8802667474992422 and parameters: {'solver': 'lbfgs', 'max_iter': 1160, 'C': 0.4202641185692597}. Best is trial 5 with value: 0.8882817243472981.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:00:22,528]\u001b[0m Trial 10 finished with value: 0.8920557913887204 and parameters: {'solver': 'sag', 'max_iter': 1530, 'C': 0.9459197403963454}. Best is trial 10 with value: 0.8920557913887204.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:00:37,512]\u001b[0m Trial 11 finished with value: 0.8920557913887204 and parameters: {'solver': 'sag', 'max_iter': 1570, 'C': 0.9939359388429903}. Best is trial 10 with value: 0.8920557913887204.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:00:52,142]\u001b[0m Trial 12 finished with value: 0.8920557913887204 and parameters: {'solver': 'sag', 'max_iter': 1390, 'C': 0.9946271134697791}. Best is trial 10 with value: 0.8920557913887204.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:01:07,136]\u001b[0m Trial 13 finished with value: 0.8923916338284329 and parameters: {'solver': 'sag', 'max_iter': 1600, 'C': 0.9850841145540543}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:01:21,365]\u001b[0m Trial 14 finished with value: 0.8902365069739235 and parameters: {'solver': 'sag', 'max_iter': 1340, 'C': 0.8453617064717462}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:01:35,904]\u001b[0m Trial 15 finished with value: 0.8908429351121891 and parameters: {'solver': 'sag', 'max_iter': 1260, 'C': 0.8771848909382964}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:01:50,430]\u001b[0m Trial 16 finished with value: 0.8905065210797695 and parameters: {'solver': 'sag', 'max_iter': 1470, 'C': 0.8225251775367826}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:02:08,288]\u001b[0m Trial 17 finished with value: 0.8681749622926095 and parameters: {'solver': 'sag', 'max_iter': 1360, 'C': 0.2378140190153618}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:02:22,479]\u001b[0m Trial 18 finished with value: 0.8889563106796116 and parameters: {'solver': 'sag', 'max_iter': 1380, 'C': 0.763582908069018}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:02:37,664]\u001b[0m Trial 19 finished with value: 0.8920557913887204 and parameters: {'solver': 'sag', 'max_iter': 1180, 'C': 0.9940474991035549}. Best is trial 13 with value: 0.8923916338284329.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best values of hyperparameters for LogisticRegression: {'solver': 'sag', 'max_iter': 1600, 'C': 0.9850841145540543}\n","f1_score on train data for LogisticRegression: 0.89\n","\n","Accuracy on test data for LogisticRegression: 0.75\n","f1_score on test data for LogisticRegression: 0.78\n"]}],"source":["# Optuna LogisticRegression\n","\n","random_state=12\n","def optuna_lr(trial):\n","  \n","  # space for hyperparameters search\n","  solver = trial.suggest_categorical(name=\"solver\", choices=[\"sag\", \"lbfgs\", \"saga\"])\n","  max_iter = trial.suggest_int(name=\"max_iter\" , low=700, high=1600, step=10)\n","  C = trial.suggest_float(name=\"C\", low=0.01, high=1.0)\n","\n","  # create and train model\n","  model = linear_model.LogisticRegression(solver=solver,\n","                                          max_iter=max_iter,\n","                                          C=C,\n","                                          random_state=random_state)\n","  model.fit(X_train, y_train)\n","  score = metrics.f1_score(y_train, model.predict(X_train))\n","\n","  return score\n","\n","# Create study object\n","# give argument direction=\"maximize\"\n","study_lr = optuna.create_study(study_name=\"LogisticRefression\", direction=\"maximize\")\n","\n","# search best combination of hypreparameters n_trials times\n","study_lr.optimize(optuna_lr, n_trials=20)\n","\n","# Get best results on train data\n","print(f\"Best values of hyperparameters for LogisticRegression: {study_lr.best_params}\")\n","print(\n","  f\"f1_score on train data for LogisticRegression: {np.round(study_lr.best_value, 2)}\"\n","  )\n","print()\n","\n","# accuracy on test data\n","model_lr_optuna = linear_model.LogisticRegression(**study_lr.best_params,random_state=random_state, )\n","model_lr_optuna.fit(X_train, y_train)\n","y_train_pred = model_lr_optuna.predict(X_train)\n","print(\n","    f\"Accuracy on test data for LogisticRegression: {np.round(model_lr_optuna.score(X_test, y_test), 2)}\")\n","y_test_pred = model_lr_optuna.predict(X_test)\n","print(f\"f1_score on test data for LogisticRegression: {np.round(metrics.f1_score(y_test, y_test_pred), 2)}\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-10-30 01:04:39,630]\u001b[0m A new study created in memory with name: RandomForestClassifier\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:41,052]\u001b[0m Trial 0 finished with value: 0.9243902439024391 and parameters: {'n_estimators': 160, 'max_depth': 34, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.9243902439024391.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:43,196]\u001b[0m Trial 1 finished with value: 0.9622411693057247 and parameters: {'n_estimators': 210, 'max_depth': 25, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.9622411693057247.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:45,772]\u001b[0m Trial 2 finished with value: 0.9749847467968273 and parameters: {'n_estimators': 250, 'max_depth': 23, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:46,772]\u001b[0m Trial 3 finished with value: 0.9335770871419867 and parameters: {'n_estimators': 110, 'max_depth': 23, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:47,594]\u001b[0m Trial 4 finished with value: 0.9242979242979242 and parameters: {'n_estimators': 90, 'max_depth': 34, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:49,782]\u001b[0m Trial 5 finished with value: 0.9235455376180324 and parameters: {'n_estimators': 250, 'max_depth': 17, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:52,354]\u001b[0m Trial 6 finished with value: 0.9460858970453853 and parameters: {'n_estimators': 270, 'max_depth': 29, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:54,326]\u001b[0m Trial 7 finished with value: 0.9458637469586374 and parameters: {'n_estimators': 220, 'max_depth': 23, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:56,802]\u001b[0m Trial 8 finished with value: 0.9317489335770871 and parameters: {'n_estimators': 280, 'max_depth': 25, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:04:58,901]\u001b[0m Trial 9 finished with value: 0.9610705596107055 and parameters: {'n_estimators': 210, 'max_depth': 22, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:00,508]\u001b[0m Trial 10 finished with value: 0.9642966127555691 and parameters: {'n_estimators': 160, 'max_depth': 15, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:02,095]\u001b[0m Trial 11 finished with value: 0.9642966127555691 and parameters: {'n_estimators': 160, 'max_depth': 15, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:03,829]\u001b[0m Trial 12 finished with value: 0.9732197200243456 and parameters: {'n_estimators': 160, 'max_depth': 19, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:05,235]\u001b[0m Trial 13 finished with value: 0.9734675205855443 and parameters: {'n_estimators': 130, 'max_depth': 29, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:06,446]\u001b[0m Trial 14 finished with value: 0.9594388533089356 and parameters: {'n_estimators': 120, 'max_depth': 40, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:08,908]\u001b[0m Trial 15 finished with value: 0.974969474969475 and parameters: {'n_estimators': 250, 'max_depth': 30, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:11,707]\u001b[0m Trial 16 finished with value: 0.959780621572212 and parameters: {'n_estimators': 300, 'max_depth': 32, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9749847467968273.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:14,066]\u001b[0m Trial 17 finished with value: 0.9762050030506407 and parameters: {'n_estimators': 240, 'max_depth': 40, 'min_samples_leaf': 3}. Best is trial 17 with value: 0.9762050030506407.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:16,458]\u001b[0m Trial 18 finished with value: 0.9600731484303565 and parameters: {'n_estimators': 240, 'max_depth': 38, 'min_samples_leaf': 4}. Best is trial 17 with value: 0.9762050030506407.\u001b[0m\n","\u001b[32m[I 2022-10-30 01:05:19,203]\u001b[0m Trial 19 finished with value: 0.9473363774733637 and parameters: {'n_estimators': 300, 'max_depth': 37, 'min_samples_leaf': 5}. Best is trial 17 with value: 0.9762050030506407.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["Best values of hyperparameters for RandomForestClassifier: {'n_estimators': 240, 'max_depth': 40, 'min_samples_leaf': 3}\n","f1_score on train data for RandomForestClassifier: 0.98\n","\n","Accuracy on test data for RandomForestClassifier: 0.78\n","f1_score on test data for RandomForestClassifier: 0.8\n"]}],"source":["# Optuna RandomForestClassifier\n","\n","random_state=12\n","def optuna_rfc(trial):\n","  \n","  # space for hyperparameters search\n","  n_estimators = trial.suggest_int(name=\"n_estimators\", low=80, high=300, step=10)\n","  max_depth = trial.suggest_int(\"max_depth\", 15, 40, 1)\n","  min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 3, 7, 1)\n","\n","  # create and train model\n","  model = ensemble.RandomForestClassifier(n_estimators=n_estimators,\n","                                          max_depth=max_depth,\n","                                          min_samples_leaf=min_samples_leaf,\n","                                          random_state=random_state)\n","  \n","  model.fit(X_train, y_train)\n","  score = metrics.f1_score(y_train, model.predict(X_train))\n","\n","  return score\n","\n","study_rfc = optuna.create_study(study_name=\"RandomForestClassifier\", direction=\"maximize\")\n","# search best combination of hypreparameters n_trials times\n","study_rfc.optimize(optuna_rfc, n_trials=20)\n","\n","# Get best results on train data\n","print(f\"Best values of hyperparameters for RandomForestClassifier: {study_rfc.best_params}\")\n","print(\n","  f\"f1_score on train data for RandomForestClassifier: {np.round(study_rfc.best_value, 2)}\"\n","  )\n","print()\n","\n","# accuracy on test data\n","model_rfc_optuna = ensemble.RandomForestClassifier(**study_rfc.best_params,random_state=random_state, )\n","model_rfc_optuna.fit(X_train, y_train)\n","y_train_pred = model_rfc_optuna.predict(X_train)\n","print(\n","    f\"Accuracy on test data for RandomForestClassifier: {np.round(model_rfc_optuna.score(X_test, y_test), 2)}\")\n","y_test_pred = model_rfc_optuna.predict(X_test)\n","print(f\"f1_score on test data for RandomForestClassifier: {np.round(metrics.f1_score(y_test, y_test_pred), 2)}\")"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('SFDS')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"98c2faa37bcfea62705d2fbc0dd7d204fd26954790a864faa64b2a1e0de25972"}}},"nbformat":4,"nbformat_minor":0}
